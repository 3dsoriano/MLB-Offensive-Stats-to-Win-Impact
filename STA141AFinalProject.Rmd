---
title: "STA 141A Final Project"
author: "Ryan Cosgrove, Daniel Soriano, and Atsushi Higuchi"
subtitle: "Work was split between Ryan Cosgrove and Daniel Soriano Only, Atsushi has been unresponsive and did not contribute to the final project"
date: "2023-12-15"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_knit$set(root.dir = "/Users/danielsoriano/Downloads/Random School Files/Stats 141")
library(ggplot2)
library(dplyr)
library(Lahman)
library(tidyr)
library(boot)
library(gridExtra)
library(grid)
```
### Introduction
In this project we attempt evaluate the hitting statistics used in baseball and to answer the question of
what hitting statistics help a baseball team win and what hitting statistics belong in the regression
model for winning percentage. We will analyze team statistics for every Major League Baseball
team in the 21st century that had a full season, so we will filter the teams from the year 2000 to
the present, excluding the year 2020 because it was a short season due to COVID-19. 

### Our Data Explained
The data we are using is from a baseball database called Lahman Database which can be imported from installing the package “Lahman” and we then use the Teams dataframe.

We will use the following team statistics:

-   `WPCT` (`Winning Percentage`) : Wins / Games
-   `BA`  (`Batting Average`) : Hits / at bats
-   `OBP` (`On Base Percentage`) : (H + BB + HBP) / (AB + BB + HBP + SF) 
-   `SLG` (`Slugging Percentage`) : (X1B + 2 * X2B + 3 * X3B + 4 * HR) / AB 
-   `OPS` (`On Base Plus Slugging Percentage`) : OBP + SLG
-   `R` : Runs scored
-   `AB` : At-Bats
-   `H` : Hits
-   `X1B` : Singles hit
-   `X2B` : Doubles hit
-   `X3B` : Triples hit
-   `HR` : Home runs hit
-   `BB` : Bases on balls or also called walks
-   `SO` : Strikeouts
-   `SB` : Stolen bases
-   `CS` : Caught stealing
-   `HBP` : Times hit by a pitch
-   `SF` : Sacrifice flies



```{r, echo= F}
teams_21stcent <- Teams %>% 
  filter(yearID > 1999 & yearID != 2020)%>%
  mutate(X1B = H - (X2B+X3B+HR),
        BA = H/AB,
        OBP = (H + BB + HBP) / (AB + BB + HBP + SF),
        SLG = (X1B + 2 * X2B + 3 * X3B + 4 * HR) / AB,
        OPS = OBP + SLG, 
        WPCT = W / G,
        BA = round(BA, digits=3),
        OBP = round(OBP, digits=3),
        SLG = round(SLG, digits=3),
        OPS = round(OPS, digits=3),
        WPCT = round(WPCT, digits=3))%>%
  dplyr::select(WPCT, BA, OBP, SLG, OPS, R, AB, H, X1B, X2B, X3B, HR, BB, SO, SB, CS, HBP, SF)

head(teams_21stcent)
attach(teams_21stcent)
```

### Regression/Correlation Coefficient

To see what statistics have the best relationship with Winning percentage, we can plot all the statistics against WPCT and add a regression line to see the relationship pattern. We also can calculate the correlation coefficient for all the statistics against WPCT and arrange them by strongest positive to strongest negative relationship.
```{r, echo = F}
ggplot(data = tidyr::pivot_longer(teams_21stcent %>%
                                    dplyr::select(WPCT, BA, OBP, SLG, OPS, R, AB, H, X1B, X2B, X3B, HR, BB, SO, SB, CS, HBP, SF), -1) , 
       aes(value, WPCT)) + 
  geom_point() + facet_wrap('name', scales = 'free')+
  geom_smooth(method = 'lm')

Correlations <- data.frame(c(cor(WPCT,BA), cor(WPCT,OBP), cor(WPCT,SLG), cor(WPCT,OPS), cor(WPCT,R), cor(WPCT,AB), cor(WPCT,H), cor(WPCT,X1B), cor(WPCT,X2B), cor(WPCT,X3B), cor(WPCT,HR), cor(WPCT,BB), cor(WPCT,SO), cor(WPCT,SB), cor(WPCT,CS), cor(WPCT,HBP), cor(WPCT,SF)))
rownames(Correlations) = c('BA', 'OBP', 'SLG', 'OPS', 'R', 'AB', 'H', 'X1B', 'X2B', 'X3B', 'HR', 'BB', 'SO', 'SB', 'CS', 'HBP', 'SF')
colnames(Correlations) = c('Cor with WPCT')

Correlations %>% arrange(desc(Correlations))
```
We can conclude that from the plots and Correlation coefficients that the statistics R, OPS, OBP, SLG have the strongest positive relationships with winning percentage of all these statistics but they aren't necessarily close to 1 indicating that none of these statistics connect perfectly to how many wins a team has. The statistics that have almost a neutral relationship with winning are AB, singles, SB, triples, strikeouts, and caught stealings, where we can conclude that just because a team performs well by these statistics doesn't mean they have a high winning percentage.

```{r, echo = F}
full = lm(WPCT ~ . , data = teams_21stcent)
summary(full)
```

Here is a plots of the statistics R, OPS, OBP, SLG that have the strongest positive relationships with winning percentage of all these statistics
```{r, echo = F}
require('ggplot2')
ggplot(teams_21stcent, aes(R + OPS + OBP + SLG, WPCT)) + 
    theme_minimal() + 
    geom_point() + 
    geom_smooth(method = 'lm', se = F)
```

### $R^2$/P-values
From looking full model, we can create an adjusted model with the variables where the p-values are less than $\alpha = 0.05$ since the p-values for each independent variable tests the $H_0$ that there is no correlation between the independent and the dependent variable. In other words, we are trying to see if we can reject the null hypothesis, $H_0 : \beta_i = 0$ Those variables would be runs (R), hits(H), at bats(AB), stolen bases (SB), caught stealing (CS), and sacrifice flies (SF).

Adjusted Model without the lowest P-Values from the full model
```{r, echo = F}
adjusted_model = lm(WPCT ~ BA + OBP + SLG + OPS + X1B + X2B + X3B + HR + BB + SO + HBP, data = teams_21stcent)
summary(adjusted_model)
```

Model with top 4 correlation coefficients (Runs, OPS, OBP, SLG)
```{r, echo =F}
highCC_model = lm(WPCT ~ R + OPS + OBP + SLG, data = teams_21stcent)
summary(highCC_model)
```
If we compare the adjusted model to the model with the highest correlation coefficients, both models do have a relatively low adjusted R squared value. But this is to be expected as R squared values measure how well the model is fitting the actual data, and from our different adjusted R-squared values, we see that the model with high the highest correlation coefficients has the lowest R-squared value, the adjusted model having a slightly higher R-squared value, and our full model having the highest R-squared value. For our 3 different model's R-squared values, roughly 32-40% of the variance found in the response variables, win percentage, can be explained by all these different predictor variables. Retrospectively, if you were able to choose any metric to predict winning percentage, it looks to be that offensive stats seem to only account for 32-40% of the game. As an athlete, this does make sense in the sport of baseball as there are many other aspects of the game where winning a game is dictated by other aspects of the game such as defense (defensive/pitching stats) or possibly even external factors such as weather conditions or different stadiums since no two MLB stadiums are the same. What we did discover is that the MLB games are not won with 50-50 offense and defense, there are more aspects to the game, as offensive stats historically only really account for at most 40% of your likely hood of winning games.

### Akaike Information Criteria/AIC

Since new predictors will always improve the model fit, even if they just marginally explain variation in the response. To penalize too many predictors in the model, an information criteria considers the fit in terms of likelihood minus the number of parameters. 

Here, $\ell$ is the log-likelihood of the data. 
```{r, echo = F}
x <- AIC(full)
y <- AIC(adjusted_model)
z <- AIC(highCC_model)
cat("AIC of the full model: ", x)
cat("\nAIC of the adjusted model: ", y)
cat("\nAIC of the highest correlation coefficient model: ", z)

a <- BIC(full)
b <- BIC(adjusted_model)
c <- BIC(highCC_model)
cat("\n\nBIC of the full model: ", a)
cat("\nBIC of the adjusted model: ", b)
cat("\nBIC of the highest correlation coefficient model: ", c)
```
After looking at log-likelihood of the data, we see that the full model had both the lowest AIC and the lowest BIC, suggesting that we would still want to use our adjusted model. We can also confirm our answer by using cross validation.

### Leave Out One Cross Validation/LOOCV
```{r, echo = F}
# LOOCV function
loocv <- function(model_formula, data) {
  predictions <- numeric(nrow(data))
  for (i in 1:nrow(data)) {
    data_leave_one_out <- data[-i, ]
    model <- lm(model_formula, data = data_leave_one_out)
    prediction <- predict(model, newdata = data[i, ])
    predictions[i] <- prediction
  }
  actual <- data$WPCT
  mse_loocv <- mean((actual - predictions)^2)
  return(mse_loocv)
}
models <- list(
  full = "WPCT ~ .",
  adjusted_model = "WPCT ~ R + AB + H + SB + CS + SF",
  highCC_model = "WPCT ~ R + OPS + OBP + SLG"
)

results <- lapply(models, function(formula) loocv(formula, teams_21stcent))
names(results) <- names(models)
results
```
Since our MSE for the full model was marginally smaller than the adjusted model, we can say that the full model is performing slightly better than the other models.

### Residual Analysis (QQ Plots)
```{r, echo = F}
residuals_full <- residuals(full)
residuals_adjusted <- residuals(adjusted_model)
residuals_highCC <- residuals(highCC_model)
#par(mfrow = c(1, 3))
qqnorm(residuals_full, main = "full")
qqline(residuals_full, col = "dodgerblue2")

qqnorm(residuals_adjusted, main = "adjusted_model")
qqline(residuals_adjusted, col = "dodgerblue2")

qqnorm(residuals_highCC, main = "highCC_model")
qqline(residuals_highCC, col = "dodgerblue2")
```

```{r, echo = F}
```

For the full residual analysis, we can see that our best model is the full model and close in second is our adjusted model. Both of which seem to be more normally distributed that our high correlation coefficient model. In the highCC_model, we see that the points are close to the line, but tail off towards the edges more than the other two plots meaning there must be a peak or a spike in our data, more specifically, we would see a spike commonly around the median values. But it probably isn't too noticeable as the highCC_model is still a very good QQ plot. 


### F-Tests
Looking at our F-Tests for our two different models:
```{r, echo = FALSE}
F_statistic_highcc = summary(highCC_model)$fstatistic[1]
df1_cc = summary(highCC_model)$fstatistic[2]
df2_cc = summary(highCC_model)$fstatistic[3]

cat("F-Statistic: ", F_statistic_highcc)
cat("Numerator Degrees of Freedom: ", df1_cc)
cat("Denominator Degrees of Freedom: ", df2_cc)

alpha <- 0.05  

critical_value_highcc <- qf(1 - alpha, df1_cc, df2_cc)
cat("Adjusted model critical value: ", critical_value_highcc)
```
High CC Model null hypothesis $H_0$: The high cc model fits the data as well as the full model.
High CC Model alternative hypothesis $H_a$: The full model fits the data significantly better than the high cc model.

Conclusion: With an F-statistic of 79.69229, 4 degrees of freedom in the numerator, and 655 in the denominator for the full model, compared to an adjusted model critical value of 2.385533, the F-statistic substantially surpasses the critical value for the adjusted model, signaling strong statistical significance. Consequently, we reject the null hypothesis $H_0$ asserting that the adjusted model fits the data as well as the full model. Instead, robust evidence supports the alternative hypothesis $H_a$ that the full model significantly surpasses the adjusted model in capturing and explaining the observed variability within the data set.

```{r, echo = FALSE}
F_statistic_adjusted = summary(adjusted_model)$fstatistic[1]
df1_a = summary(adjusted_model)$fstatistic[2]
df2_a = summary(adjusted_model)$fstatistic[3]

cat("F-Statistic: ", F_statistic_adjusted)
cat("Numerator Degrees of Freedom: ", df1_a)
cat("Denominator Degrees of Freedom: ", df2_a)

alpha <- 0.05
critical_value_adjusted <- qf(1 - alpha, df1_a, df2_a)
cat("Adjusted model critical value: ", critical_value_adjusted)
```
Adjusted Model null hypothesis $H_0$: The adjusted model fits the data as well as the full model.
Adjusted alternative hypothesis $H_a$: The full model fits the data significantly better than the adjusted model.

Conclusion: Given the calculated F-statistic of 67.16695 exceeding the critical value of 2.112446, the outcome leads to rejecting the null hypothesis $H_0$ in favor of the alternative hypothesis $H_a$ that the adjusted model is statistically significant at the significance level $\alpha = 0.05$. The adjusted model substantially explains the data set's variation. This stark difference confirms that the chosen variables wield considerable collective influence on the dependent variable. The statistical evidence strongly supports the relevance of these predictors, affirming their crucial role in modeling the data effectively.


```{r, echo = F}
perform_F_test <- function(variable) {
 full = lm(WPCT ~ . , data = teams_21stcent %>% select(variable, WPCT))
 f_test_result = data.frame(statistic = summary(full)$fstatistic[1],
                             df1 = summary(full)$fstatistic[2],
                             df2 = summary(full)$fstatistic[3])
 return(f_test_result)
}


f_test_results = list()

variables = c('BA', 'OBP', 'SLG', 'OPS', 'R', 'AB', 'H', 'X1B', 'X2B', 'X3B', 'HR', 'BB', 'SO', 'SB', 'CS', 'HBP', 'SF')

for (variable in variables) {
 f_test_results[[variable]] = perform_F_test(variable)
}

df1_z = summary(full)$fstatistic[2]
df2_z = summary(full)$fstatistic[3]
critical_value_adjusted <- qf(0.95, df1_z, df2_z)
cat("Critical value for all models: ", critical_value_adjusted)
cat("\n")
print(f_test_results)
```
If we wanted to look at each stat individually for their effect on Winning Percentage, it's evident that across the various offensive baseball statistics we observed, a majority of the the calculated F-statistics significantly surpass the critical value of 1.659253. This compelling finding indicates strong statistical significance for these statistics within their respective models in which they are the only variable. In essence, each of these individual statistics demonstrates substantial explanatory power in relation to the variability observed in the data set. Such significance underscores the importance of these offensive baseball metrics in understanding and predicting performance outcomes, thereby emphasizing their relevance in baseball analytics and player performance evaluation.

Of all the offensive baseball statistics, the F-statistic for the Stolen Bases ($SB$) model falls below the critical value. It suggests that, in this particular context, the model might not possess enough collective explanatory power to significantly predict or explain the variability in the data set pertaining to stolen bases. This outcome indicates that the variables or factors included in this model, when considered together, might not adequately capture the nuances or influences affecting stolen base outcomes in the analyzed data set. It's essential to note that while this model might not exhibit significant explanatory capability in this specific instance, it doesn't diminish the importance of stolen bases as a metric in baseball.




```{r, ref.label=knitr::all_labels(),echo=TRUE,eval=FALSE,tidy=F}

```